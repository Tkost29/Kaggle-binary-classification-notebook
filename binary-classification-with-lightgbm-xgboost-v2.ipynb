{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"},{"sourceId":9293783,"sourceType":"datasetVersion","datasetId":5626665}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:06:23.917115Z","iopub.execute_input":"2025-08-31T17:06:23.917362Z","iopub.status.idle":"2025-08-31T17:06:25.812065Z","shell.execute_reply.started":"2025-08-31T17:06:23.917335Z","shell.execute_reply":"2025-08-31T17:06:25.811250Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e8/sample_submission.csv\n/kaggle/input/playground-series-s5e8/train.csv\n/kaggle/input/playground-series-s5e8/test.csv\n/kaggle/input/bank-marketing-dataset-full/bank-full.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# === Prep: データ読み込み・前処理・CV・LGBMパラメータ定義 ===\nimport numpy as np, pandas as pd, lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\n\n# --- Load ---\nPATH = \"/kaggle/input/playground-series-s5e8/\"\ntrain = pd.read_csv(PATH + \"train.csv\", index_col=\"id\")\ntest  = pd.read_csv(PATH + \"test.csv\",  index_col=\"id\")\n\nTARGET = \"y\"\nCATS = ['job','marital','education','default','housing','loan','contact','month','poutcome']\nNUMS = ['age','balance','day','duration','campaign','pdays','previous']\n\n# --- 軽いwinsorize（任意：過度な外れ値を弱めて安定化） ---\ndef winsorize(df, cols, lo=0.005, hi=0.995):\n    df = df.copy()\n    for c in cols:\n        lo_b = df[c].quantile(lo); hi_b = df[c].quantile(hi)\n        df[c] = df[c].clip(lo_b, hi_b)\n    return df\ntrain[NUMS] = winsorize(train, NUMS, 0.005, 0.995)[NUMS]\ntest[NUMS]  = winsorize(test,  NUMS, 0.005, 0.995)[NUMS]\n\n# --- Categorical を train+testでレベル合わせして category 型へ ---\nfor c in CATS:\n    all_vals = pd.concat([train[c].astype(str), test[c].astype(str)], axis=0)\n    cats = pd.Categorical(all_vals).categories\n    train[c] = pd.Categorical(train[c].astype(str), categories=cats)\n    test[c]  = pd.Categorical(test[c].astype(str),  categories=cats)\n\n# --- 特徴/目的変数 ---\ny = train[TARGET].astype(int).values     # numpy にしておく（fit_predict内の y[idx] が安全に動く）\nX_full = train.drop(columns=[TARGET])\nX_test = test.copy()\n\n# --- CV folds ---\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\nfolds = list(skf.split(X_full, y))\n\n# --- 不均衡対策 ---\nneg = int((y == 0).sum())\npos = int((y == 1).sum())\nspw = neg / max(pos, 1)\nprint(f\"scale_pos_weight = {spw:.4f}  (neg={neg}, pos={pos})\")\nprint(\"Prep done. shapes:\", X_full.shape, X_test.shape)\n\n# --- LGBM パラメータ（A=GBDT / B=GOSS / C=DART） ---\nbase_common = dict(\n    objective=\"binary\",\n    metric=\"auc\",\n    learning_rate=0.03,\n    num_leaves=192,\n    max_depth=-1,\n    min_data_in_leaf=24,\n    lambda_l1=0.5,\n    lambda_l2=3.0,\n    feature_fraction=0.70,\n    bagging_fraction=0.80,\n    bagging_freq=1,\n    max_bin=8192,\n    scale_pos_weight=spw,\n    force_row_wise=True,\n    seed=42,\n    max_cat_to_onehot=12,\n    max_cat_threshold=128,\n)\n\nparams_A = dict(boosting=\"gbdt\", **base_common)\n\n# GOSS は bagging を使わない（指定しても無視されるが警告低減のため外す）\nparams_B = dict(\n    boosting=\"goss\",\n    objective=\"binary\",\n    metric=\"auc\",\n    learning_rate=0.03,\n    num_leaves=128,\n    max_depth=-1,\n    min_data_in_leaf=32,\n    lambda_l1=0.5,\n    lambda_l2=2.0,\n    feature_fraction=0.70,\n    max_bin=4096,\n    scale_pos_weight=spw,\n    force_row_wise=True,\n    seed=42,\n    max_cat_to_onehot=12,\n    max_cat_threshold=128,\n)\n\n# DART（多様性用・重みは少なめで混ぜる想定）\nparams_C = base_common.copy()\nparams_C.update({\n    \"boosting\": \"dart\",\n    \"learning_rate\": 0.05,   # base_commonの0.03を上書き\n    \"drop_rate\": 0.10,\n    \"skip_drop\": 0.5,\n    \"max_drop\": 50,\n    \"xgboost_dart_mode\": True,\n    \"uniform_drop\": False,\n})\n# 早期終了と最大ラウンドの既定（Hotfix側で使います）\nEARLY = 400\nMAX_ROUNDS = 40000\n\n# 外部 bank-full はまず混ぜない（Hotfix側の条件分岐を通さない）\nORIG_FRAC = 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:06:25.813636Z","iopub.execute_input":"2025-08-31T17:06:25.813989Z","iopub.status.idle":"2025-08-31T17:06:35.849718Z","shell.execute_reply.started":"2025-08-31T17:06:25.813968Z","shell.execute_reply":"2025-08-31T17:06:35.849005Z"}},"outputs":[{"name":"stdout","text":"scale_pos_weight = 7.2884  (neg=659512, pos=90488)\nPrep done. shapes: (750000, 16) (250000, 16)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === Hotfix: fit_predict 定義 + 学習実行 + ブレンド & 提出 ===\nimport gc, numpy as np, pandas as pd, lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score, log_loss\n\n# 推奨：外部 bank-full をまずは混ぜない（分布差で鈍るケースが多い）\ntry:\n    ORIG_FRAC\nexcept NameError:\n    ORIG_FRAC = 0.0\nelse:\n    ORIG_FRAC = float(ORIG_FRAC)\n\n# 既存の params に categorical_column が入っていたら削除（Dataset側指定で十分）\nfor _pname in [\"params_A\", \"params_B\", \"params_C\"]:\n    if _pname in globals():\n        globals()[_pname].pop(\"categorical_column\", None)\n        # 小カテゴリはワンホット化（微増しやすい）\n        globals()[_pname].update(dict(max_cat_to_onehot=12, max_cat_threshold=128))\n\n# 早期終了の既定\ntry:\n    EARLY\nexcept NameError:\n    EARLY = 400\ntry:\n    MAX_ROUNDS\nexcept NameError:\n    MAX_ROUNDS = 40000\ntry:\n    N_SPLITS\nexcept NameError:\n    N_SPLITS = 5\n\n# 必須オブジェクトの存在チェック（未定義ならここでエラーにします）\nassert \"X_full\" in globals() and \"X_test\" in globals() and \"y\" in globals(), \"X_full/X_test/y が未定義です。前セルを実行してください。\"\nassert \"folds\" in globals(), \"folds が未定義です。CV 分割のセルを実行してください。\"\nassert \"CATS\" in globals(), \"CATS が未定義です。前処理セルを実行してください。\"\n\ndef fit_predict(params, X, y, X_test, folds, early_stopping=400, label=\"MODEL\"):\n    \"\"\"LightGBM を CV 学習して OOF / Test を返す（外部データ混入なし版）\"\"\"\n    import gc\n    from sklearn.metrics import roc_auc_score, log_loss\n\n    oof  = np.zeros(len(X), dtype=float)\n    pred = np.zeros(len(X_test), dtype=float)\n\n    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n        dtr = lgb.Dataset(X_tr, label=y_tr, categorical_feature=CATS, free_raw_data=False)\n        dvl = lgb.Dataset(X_va, label=y_va, categorical_feature=CATS, free_raw_data=False)\n\n        model = lgb.train(\n            params=params,\n            train_set=dtr,\n            num_boost_round=MAX_ROUNDS,          # 例: 40000\n            valid_sets=[dtr, dvl],\n            valid_names=[\"train\",\"valid\"],\n            callbacks=[lgb.early_stopping(early_stopping),  # 例: 400\n                       lgb.log_evaluation(1000)],\n        )\n\n        p_va = model.predict(X_va,   num_iteration=model.best_iteration)\n        p_te = model.predict(X_test, num_iteration=model.best_iteration)\n\n        oof[va_idx] = p_va\n        pred       += p_te / len(folds)\n\n        auc = roc_auc_score(y_va, p_va)\n        ll  = log_loss(y_va, np.clip(p_va, 1e-15, 1-1e-15))\n        print(f\"[{label} Fold {i}] AUC={auc:.6f}  BestIter={model.best_iteration}\")\n\n        del model, dtr, dvl, X_tr, X_va\n        gc.collect()\n\n    print(f\"[{label}] OOF AUC={roc_auc_score(y, oof):.6f}\")\n    return oof, pred\n\n\n# === Seed Bagging: 同じ設定でseedだけ変えて平均化 ===\nfrom sklearn.metrics import roc_auc_score\n\ndef fit_predict_seedbag(params, X_full, y, X_test, folds, EARLY, label=\"A-GBDT\",\n                        seeds=(42, 77, 2024)):\n    \"\"\"params を seeds の回数ぶん走らせ、OOF/TEST を平均して返す\"\"\"\n    oofs, preds = [], []\n    for sd in seeds:\n        p = params.copy()\n        # 乱数種を差し替え（bagging/feature_fractionのランダム性を活かす）\n        p[\"seed\"] = sd\n        p[\"bagging_seed\"] = sd\n        p[\"feature_fraction_seed\"] = sd\n\n        oof_i, pred_i = fit_predict(p, X_full, y, X_test, folds, EARLY, f\"{label}-s{sd}\")\n        oofs.append(oof_i)\n        preds.append(pred_i)\n\n    oof_mean  = np.mean(oofs, axis=0)\n    pred_mean = np.mean(preds, axis=0)\n    try:\n        print(f\"[{label} seedbag] OOF AUC={roc_auc_score(y, oof_mean):.6f}\")\n    except Exception:\n        pass\n    return oof_mean, pred_mean\n# --- モデルを用意（A/B は必須、C はあれば使う） ---\nmodels = []\nassert \"params_A\" in globals() and \"params_B\" in globals(), \"params_A / params_B が未定義です。\"\nmodels.append((\"A-GBDT\", params_A))\nmodels.append((\"B-GOSS\", params_B))\n#if \"params_C\" in globals():\n#    models.append((\"C-DART\", params_C))\n\n# --- 学習 ---\noofs, preds, labels = [], [], []\nfor lbl, prm in models:\n    oof_m, pred_m = fit_predict(prm, X_full, y, X_test, folds, EARLY, lbl)\n    oofs.append(oof_m); preds.append(pred_m); labels.append(lbl)\n\n# --- OOF で最適重み探索（2本 or 3本に対応） ---\ndef best_weights(oofs, y, labels):\n    k = len(oofs)\n    if k == 2:\n        best_auc, best = -1.0, None\n        for w in np.linspace(0, 1, 51):\n            mix = w*oofs[0] + (1-w)*oofs[1]\n            auc = roc_auc_score(y, mix)\n            if auc > best_auc:\n                best_auc, best = auc, (w, 1-w)\n        return best_auc, best\n    elif k == 3:\n        best_auc, best = -1.0, None\n        for wA in np.linspace(0, 1, 26):\n            for wB in np.linspace(0, 1-wA, 26):\n                wC = 1.0 - wA - wB\n                mix = wA*oofs[0] + wB*oofs[1] + wC*oofs[2]\n                auc = roc_auc_score(y, mix)\n                if auc > best_auc:\n                    best_auc, best = auc, (wA, wB, wC)\n        return best_auc, best\n    else:\n        raise ValueError(\"この簡易探索は 2 or 3 本モデルのみ対応です。\")\n\nbest_auc, w = best_weights(oofs, y, labels)\nprint(\"\\n[Blend search] best OOF AUC=\", best_auc, \"  weights=\", dict(zip(labels, w)))\n\n# --- ブレンド & 提出 ---\nblend_oof  = np.zeros_like(oofs[0])\nblend_pred = np.zeros_like(preds[0])\nfor wi, o in zip(w, oofs):\n    blend_oof += wi * o\nfor wi, p in zip(w, preds):\n    blend_pred += wi * p\n\nprint(\"=\"*60)\nprint(\"Per-model OOF:\")\nfor lbl, o in zip(labels, oofs):\n    print(f\"  {lbl:7s} AUC={roc_auc_score(y, o):.6f}  LL={log_loss(y, np.clip(o,1e-15,1-1e-15)):.6f}\")\nprint(f\"BLEND   AUC={roc_auc_score(y, blend_oof):.6f}  LL={log_loss(y, np.clip(blend_oof,1e-15,1-1e-15)):.6f}\")\n\nsub = pd.DataFrame({\"id\": X_test.index, \"y\": blend_pred})\nsub.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"Saved: /kaggle/working/submission.csv  shape:\", sub.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T17:06:35.850574Z","iopub.execute_input":"2025-08-31T17:06:35.850795Z","iopub.status.idle":"2025-08-31T18:02:01.405728Z","shell.execute_reply.started":"2025-08-31T17:06:35.850776Z","shell.execute_reply":"2025-08-31T18:02:01.404981Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Total Bins 6967\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.985154\tvalid's auc: 0.973268\n[2000]\ttrain's auc: 0.991972\tvalid's auc: 0.973832\nEarly stopping, best iteration is:\n[2366]\ttrain's auc: 0.993565\tvalid's auc: 0.973868\n[A-GBDT Fold 1] AUC=0.973868  BestIter=2366\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Total Bins 6947\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.985375\tvalid's auc: 0.97204\n[2000]\ttrain's auc: 0.992092\tvalid's auc: 0.972508\nEarly stopping, best iteration is:\n[2456]\ttrain's auc: 0.99399\tvalid's auc: 0.972529\n[A-GBDT Fold 2] AUC=0.972529  BestIter=2456\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Total Bins 6955\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.98534\tvalid's auc: 0.971925\n[2000]\ttrain's auc: 0.992146\tvalid's auc: 0.97243\nEarly stopping, best iteration is:\n[2271]\ttrain's auc: 0.993307\tvalid's auc: 0.972443\n[A-GBDT Fold 3] AUC=0.972443  BestIter=2271\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Total Bins 6988\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.98533\tvalid's auc: 0.972961\n[2000]\ttrain's auc: 0.992049\tvalid's auc: 0.97337\nEarly stopping, best iteration is:\n[2020]\ttrain's auc: 0.992144\tvalid's auc: 0.973377\n[A-GBDT Fold 4] AUC=0.973377  BestIter=2020\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Total Bins 6961\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.985313\tvalid's auc: 0.972242\n[2000]\ttrain's auc: 0.99205\tvalid's auc: 0.972757\n[3000]\ttrain's auc: 0.995617\tvalid's auc: 0.972809\nEarly stopping, best iteration is:\n[2773]\ttrain's auc: 0.994992\tvalid's auc: 0.972826\n[A-GBDT Fold 5] AUC=0.972826  BestIter=2773\n[A-GBDT] OOF AUC=0.972985\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Total Bins 5610\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] Using GOSS\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.981957\tvalid's auc: 0.972955\n[2000]\ttrain's auc: 0.9884\tvalid's auc: 0.973478\nEarly stopping, best iteration is:\n[2554]\ttrain's auc: 0.990758\tvalid's auc: 0.973555\n[B-GOSS Fold 1] AUC=0.973555  BestIter=2554\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Total Bins 5622\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] Using GOSS\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.982153\tvalid's auc: 0.971753\n[2000]\ttrain's auc: 0.988611\tvalid's auc: 0.972367\n[3000]\ttrain's auc: 0.992426\tvalid's auc: 0.972478\nEarly stopping, best iteration is:\n[2727]\ttrain's auc: 0.991548\tvalid's auc: 0.972496\n[B-GOSS Fold 2] AUC=0.972496  BestIter=2727\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Total Bins 5620\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] Using GOSS\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.982168\tvalid's auc: 0.971726\n[2000]\ttrain's auc: 0.988623\tvalid's auc: 0.972165\n[3000]\ttrain's auc: 0.992426\tvalid's auc: 0.972124\nEarly stopping, best iteration is:\n[2609]\ttrain's auc: 0.991167\tvalid's auc: 0.972205\n[B-GOSS Fold 3] AUC=0.972205  BestIter=2609\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Total Bins 5628\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] Using GOSS\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.982026\tvalid's auc: 0.97249\n[2000]\ttrain's auc: 0.988472\tvalid's auc: 0.973006\n[3000]\ttrain's auc: 0.992262\tvalid's auc: 0.973068\nEarly stopping, best iteration is:\n[2653]\ttrain's auc: 0.991168\tvalid's auc: 0.973098\n[B-GOSS Fold 4] AUC=0.973098  BestIter=2653\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Total Bins 5609\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] Using GOSS\n[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 400 rounds\n[1000]\ttrain's auc: 0.982154\tvalid's auc: 0.971934\n[2000]\ttrain's auc: 0.988485\tvalid's auc: 0.9725\n[3000]\ttrain's auc: 0.992311\tvalid's auc: 0.972635\nEarly stopping, best iteration is:\n[3220]\ttrain's auc: 0.992928\tvalid's auc: 0.972668\n[B-GOSS Fold 5] AUC=0.972668  BestIter=3220\n[B-GOSS] OOF AUC=0.972790\n\n[Blend search] best OOF AUC= 0.9731276293623357   weights= {'A-GBDT': 0.6, 'B-GOSS': 0.4}\n============================================================\nPer-model OOF:\n  A-GBDT  AUC=0.972985  LL=0.176738\n  B-GOSS  AUC=0.972790  LL=0.186042\nBLEND   AUC=0.973128  LL=0.179417\nSaved: /kaggle/working/submission.csv  shape: (250000, 2)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ================================\n# XGBoost depthwise + lossguide（外部 bank-full 併用可）フル版\n# ================================\nimport os, gc, numpy as np, pandas as pd, xgboost as xgb\nfrom itertools import combinations\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, log_loss\n\n# ---------- 1) データ読込 ----------\nPATH = \"/kaggle/input/playground-series-s5e8/\"\ntrain = pd.read_csv(PATH + \"train.csv\", index_col=\"id\")\ntest  = pd.read_csv(PATH + \"test.csv\",  index_col=\"id\")\n\nTARGET = \"y\"\nNUMS = ['age','balance','day','duration','campaign','pdays','previous']\nCATS = ['job','marital','education','default','housing','loan','contact','month','poutcome']\n\n# 外部データ（任意）\norig = None\nfor p in [\n    \"/kaggle/input/bank-marketing-dataset-full/bank-full.csv\",\n    \"/kaggle/input/bank-marketing-datasets/bank-full.csv\",\n    \"/kaggle/input/uci-bank-marketing-dataset/bank-full.csv\",\n    \"/kaggle/input/bank-marketing-dataset/bank-full.csv\",\n]:\n    if os.path.exists(p):\n        o = pd.read_csv(p, delimiter=\";\")\n        o[\"y\"] = o[\"y\"].map({\"yes\":1, \"no\":0}).astype(int)\n        # 必要列のみ揃える（外部が余分な列を持っていても安全）\n        keep = [c for c in (NUMS+CATS+[TARGET]) if c in o.columns]\n        orig = o[keep].copy()\n        break\nUSE_ORIG = orig is not None\nif USE_ORIG:\n    print(f\"Found external: {p}  shape={orig.shape}\")\nelse:\n    print(\"No external bank-full found. Proceeding without it.\")\n\n# 文字列化（ペア符号化のため）\nfor df in (train, test) + ((orig,) if USE_ORIG else ()):\n    for c in CATS:\n        df[c] = df[c].astype(str)\n\n# ---------- 2) ペア特徴（全ペア）を factorize で一括作成 ----------\ndef build_pairwise_codes(train, test, orig, use_orig, num_cols, cat_cols):\n    cols_all = num_cols + cat_cols\n    PAIR_COLS = []\n    new_tr, new_te = {}, {}\n    new_or = {} if use_orig else None\n\n    for a, b in combinations(cols_all, 2):\n        name = f\"{a}-{b}\"\n        tr = (train[a].astype(str).values + \"_\" + train[b].astype(str).values)\n        te = (test[a].astype(str).values  + \"_\" + test[b].astype(str).values)\n        if use_orig:\n            orv = (orig[a].astype(str).values + \"_\" + orig[b].astype(str).values)\n            combo = np.concatenate([tr, te, orv])\n            codes = pd.factorize(combo)[0].astype(\"int32\")\n            ntr, nte = len(tr), len(te)\n            new_tr[name] = codes[:ntr]\n            new_te[name] = codes[ntr:ntr+nte]\n            new_or[name] = codes[ntr+nte:]\n        else:\n            combo = np.concatenate([tr, te])\n            codes = pd.factorize(combo)[0].astype(\"int32\")\n            ntr = len(tr)\n            new_tr[name] = codes[:ntr]\n            new_te[name] = codes[ntr:]\n        PAIR_COLS.append(name)\n\n    train2 = pd.concat([train, pd.DataFrame(new_tr, index=train.index)], axis=1)\n    test2  = pd.concat([test,  pd.DataFrame(new_te, index=test.index)], axis=1)\n    orig2  = None\n    if use_orig:\n        orig2 = pd.concat([orig, pd.DataFrame(new_or, index=orig.index)], axis=1)\n\n    # デフラグ\n    return train2.copy(), test2.copy(), (orig2.copy() if use_orig else None), PAIR_COLS\n\ntrain2, test2, orig2, PAIR_COLS = build_pairwise_codes(train, test, orig, USE_ORIG, NUMS, CATS)\nFEATURES = [c for c in train2.columns if c != TARGET]\nprint(f\"train/test shapes: {train2.shape} {test2.shape}\")\n\n# ---------- 3) Count-encoding を一括適用 ----------\ndef count_encode_bulk(tr, va, te, cols, use_log1p=True):\n    ce_tr, ce_va, ce_te = {}, {}, {}\n    for c in cols:\n        vc = tr[c].value_counts()\n        s_tr = tr[c].map(vc).fillna(0)\n        s_va = va[c].map(vc).fillna(0)\n        s_te = te[c].map(vc).fillna(0)\n        if use_log1p:\n            s_tr = np.log1p(s_tr)\n            s_va = np.log1p(s_va)\n            s_te = np.log1p(s_te)\n        ce_tr[f\"CE_{c}\"] = s_tr.astype(\"float32\").values\n        ce_va[f\"CE_{c}\"] = s_va.astype(\"float32\").values\n        ce_te[f\"CE_{c}\"] = s_te.astype(\"float32\").values\n    tr_out = pd.concat([tr.drop(columns=cols, errors=\"ignore\"),\n                        pd.DataFrame(ce_tr, index=tr.index)], axis=1)\n    va_out = pd.concat([va.drop(columns=cols, errors=\"ignore\"),\n                        pd.DataFrame(ce_va, index=va.index)], axis=1)\n    te_out = pd.concat([te.drop(columns=cols, errors=\"ignore\"),\n                        pd.DataFrame(ce_te, index=te.index)], axis=1)\n    return tr_out, va_out, te_out\n\n# ---------- 4) CV 設定 ----------\nN_FOLDS = 6            # ★ スコア引き上げ狙いで 10-fold\nSEED    = 42\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# ---------- 5) XGBoost パラメータ（depthwise と lossguide）----------\nXGB_PARAMS = dict(\n    n_estimators=20000,\n    learning_rate=0.010,\n    objective=\"binary:logistic\",\n    eval_metric=[\"auc\",\"logloss\"],\n    max_depth=7,\n    min_child_weight=8,\n    gamma=1.0,\n    subsample=0.85,\n    colsample_bytree=0.50,\n    reg_lambda=4.0,\n    reg_alpha=3.0,\n    max_bin=1024,             # ★ 分割の解像度を上げる\n    random_state=SEED,\n    n_jobs=-1,\n    tree_method=\"hist\",\n    enable_categorical=False,\n    early_stopping_rounds=1500\n)\n\n# 衝突しうるキーを除外してから上書き\nXGB_PARAMS_LOSSGUIDE = dict(\n    **{k:v for k,v in XGB_PARAMS.items()\n       if k not in [\"max_depth\",\"min_child_weight\",\"gamma\"]},\n    grow_policy=\"lossguide\",\n    max_depth=0,                # lossguide は深さ固定なし\n    max_leaves=192,             # 表現力UP（必要なら 128～256 の間で微調整）\n    min_child_weight=14,        # やや強めに\n    gamma=0.5\n)\n\n# ---------- 6) 学習関数 ----------\ndef run_cv(params, name=\"XGB\"):\n    oof = np.zeros(len(train2), dtype=float)\n    pred = np.zeros(len(test2), dtype=float)\n\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(train2, train2[TARGET]), 1):\n        tr_df = train2.iloc[tr_idx].copy()\n        va_df = train2.iloc[va_idx].copy()\n        te_df = test2.copy()\n\n        # 補助データを結合\n        if USE_ORIG and orig2 is not None:\n            add_cols = [c for c in FEATURES if c in orig2.columns]\n            X_tr = pd.concat([tr_df[add_cols], orig2[add_cols]], ignore_index=True)\n            y_tr = np.concatenate([tr_df[TARGET].values, orig2[TARGET].values])\n        else:\n            add_cols = FEATURES\n            X_tr = tr_df[add_cols]\n            y_tr = tr_df[TARGET].values\n\n        X_va = va_df[add_cols]; y_va = va_df[TARGET].values\n        X_te = te_df[add_cols]\n\n        # Count-Enc（PAIR_COLS + CATS）を学習foldで作成\n        ce_cols = [c for c in (PAIR_COLS + CATS) if c in add_cols]\n        X_tr, X_va, X_te = count_encode_bulk(X_tr, X_va, X_te, ce_cols, use_log1p=True)\n\n        model = xgb.XGBClassifier(**params)\n        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=1000)\n\n        # 予測（best_iteration を尊重）\n        best_it = getattr(model, \"best_iteration\", None)\n        try:\n            p_va = model.predict_proba(X_va, iteration_range=(0, best_it+1))[:, 1]\n            p_te = model.predict_proba(X_te, iteration_range=(0, best_it+1))[:, 1]\n        except Exception:\n            p_va = model.predict_proba(X_va)[:, 1]\n            p_te = model.predict_proba(X_te)[:, 1]\n\n        oof[va_idx] = p_va\n        pred += p_te / skf.n_splits\n\n        auc = roc_auc_score(y_va, p_va); ll = log_loss(y_va, p_va, eps=1e-15)\n        print(f\"[{name} Fold {fold}] AUC={auc:.6f}  LogLoss={ll:.6f}  best_it={best_it}\")\n\n        del model, tr_df, va_df, te_df, X_tr, X_va, X_te\n        gc.collect()\n\n    return oof, pred\n\n# ---------- 7) 2本学習 → 平均 ----------\nprint(f\"========== Seed {SEED} / {N_FOLDS}-fold ==========\")\noof_A, pred_A = run_cv(XGB_PARAMS,            name=\"XGB-depthwise\")\noof_B, pred_B = run_cv(XGB_PARAMS_LOSSGUIDE,  name=\"XGB-lossguide\")\n\noof_blend  = 0.5*oof_A + 0.5*oof_B\npred_blend = 0.5*pred_A + 0.5*pred_B\n\nprint(\"=\"*60)\nprint(\"OOF AUC (depthwise) =\", roc_auc_score(train2[TARGET], oof_A))\nprint(\"OOF AUC (lossguide) =\", roc_auc_score(train2[TARGET], oof_B))\nprint(\"OOF AUC (blend)     =\", roc_auc_score(train2[TARGET], oof_blend))\n\n# ---------- 8) 保存 ----------\npd.DataFrame({\"xgb_oof\": oof_blend}).to_csv(\"/kaggle/working/xgb_oof.csv\", index=False)\npd.DataFrame({\"xgb_pred\": pred_blend}).to_csv(\"/kaggle/working/xgb_pred.csv\", index=False)\npd.DataFrame({\"id\": test2.index, \"y\": pred_blend}).to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"Saved: submission.csv / xgb_oof.csv / xgb_pred.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:02:01.406673Z","iopub.execute_input":"2025-08-31T18:02:01.406911Z","iopub.status.idle":"2025-09-01T02:37:01.419853Z","shell.execute_reply.started":"2025-08-31T18:02:01.406892Z","shell.execute_reply":"2025-09-01T02:37:01.419054Z"}},"outputs":[{"name":"stdout","text":"Found external: /kaggle/input/bank-marketing-dataset-full/bank-full.csv  shape=(45211, 17)\ntrain/test shapes: (750000, 137) (250000, 136)\n========== Seed 42 / 6-fold ==========\n[0]\tvalidation_0-auc:0.94836\tvalidation_0-logloss:0.37762\n[1000]\tvalidation_0-auc:0.96936\tvalidation_0-logloss:0.14260\n[2000]\tvalidation_0-auc:0.97170\tvalidation_0-logloss:0.13708\n[3000]\tvalidation_0-auc:0.97274\tvalidation_0-logloss:0.13454\n[4000]\tvalidation_0-auc:0.97335\tvalidation_0-logloss:0.13298\n[5000]\tvalidation_0-auc:0.97373\tvalidation_0-logloss:0.13196\n[6000]\tvalidation_0-auc:0.97399\tvalidation_0-logloss:0.13124\n[7000]\tvalidation_0-auc:0.97415\tvalidation_0-logloss:0.13076\n[8000]\tvalidation_0-auc:0.97426\tvalidation_0-logloss:0.13043\n[9000]\tvalidation_0-auc:0.97433\tvalidation_0-logloss:0.13021\n[10000]\tvalidation_0-auc:0.97438\tvalidation_0-logloss:0.13006\n[11000]\tvalidation_0-auc:0.97440\tvalidation_0-logloss:0.12998\n[12000]\tvalidation_0-auc:0.97440\tvalidation_0-logloss:0.12995\n[13000]\tvalidation_0-auc:0.97440\tvalidation_0-logloss:0.12994\n[14000]\tvalidation_0-auc:0.97440\tvalidation_0-logloss:0.12995\n[14169]\tvalidation_0-auc:0.97440\tvalidation_0-logloss:0.12995\n[XGB-depthwise Fold 1] AUC=0.974405  LogLoss=0.129937  best_it=12670\n[0]\tvalidation_0-auc:0.94898\tvalidation_0-logloss:0.37761\n[1000]\tvalidation_0-auc:0.96838\tvalidation_0-logloss:0.14452\n[2000]\tvalidation_0-auc:0.97099\tvalidation_0-logloss:0.13865\n[3000]\tvalidation_0-auc:0.97210\tvalidation_0-logloss:0.13602\n[4000]\tvalidation_0-auc:0.97270\tvalidation_0-logloss:0.13452\n[5000]\tvalidation_0-auc:0.97307\tvalidation_0-logloss:0.13358\n[6000]\tvalidation_0-auc:0.97333\tvalidation_0-logloss:0.13291\n[7000]\tvalidation_0-auc:0.97351\tvalidation_0-logloss:0.13242\n[8000]\tvalidation_0-auc:0.97364\tvalidation_0-logloss:0.13205\n[9000]\tvalidation_0-auc:0.97373\tvalidation_0-logloss:0.13180\n[10000]\tvalidation_0-auc:0.97379\tvalidation_0-logloss:0.13163\n[11000]\tvalidation_0-auc:0.97384\tvalidation_0-logloss:0.13150\n[12000]\tvalidation_0-auc:0.97385\tvalidation_0-logloss:0.13145\n[13000]\tvalidation_0-auc:0.97386\tvalidation_0-logloss:0.13143\n[14000]\tvalidation_0-auc:0.97386\tvalidation_0-logloss:0.13143\n[14848]\tvalidation_0-auc:0.97387\tvalidation_0-logloss:0.13142\n[XGB-depthwise Fold 2] AUC=0.973867  LogLoss=0.131418  best_it=13349\n[0]\tvalidation_0-auc:0.94699\tvalidation_0-logloss:0.37764\n[1000]\tvalidation_0-auc:0.96772\tvalidation_0-logloss:0.14566\n[2000]\tvalidation_0-auc:0.97015\tvalidation_0-logloss:0.14014\n[3000]\tvalidation_0-auc:0.97126\tvalidation_0-logloss:0.13758\n[4000]\tvalidation_0-auc:0.97185\tvalidation_0-logloss:0.13616\n[5000]\tvalidation_0-auc:0.97227\tvalidation_0-logloss:0.13514\n[6000]\tvalidation_0-auc:0.97254\tvalidation_0-logloss:0.13447\n[7000]\tvalidation_0-auc:0.97273\tvalidation_0-logloss:0.13398\n[8000]\tvalidation_0-auc:0.97286\tvalidation_0-logloss:0.13367\n[9000]\tvalidation_0-auc:0.97295\tvalidation_0-logloss:0.13343\n[10000]\tvalidation_0-auc:0.97301\tvalidation_0-logloss:0.13329\n[11000]\tvalidation_0-auc:0.97306\tvalidation_0-logloss:0.13318\n[12000]\tvalidation_0-auc:0.97307\tvalidation_0-logloss:0.13316\n[13000]\tvalidation_0-auc:0.97308\tvalidation_0-logloss:0.13316\n[13185]\tvalidation_0-auc:0.97309\tvalidation_0-logloss:0.13315\n[XGB-depthwise Fold 3] AUC=0.973077  LogLoss=0.133146  best_it=11686\n[0]\tvalidation_0-auc:0.94889\tvalidation_0-logloss:0.37760\n[1000]\tvalidation_0-auc:0.96881\tvalidation_0-logloss:0.14363\n[2000]\tvalidation_0-auc:0.97123\tvalidation_0-logloss:0.13810\n[3000]\tvalidation_0-auc:0.97224\tvalidation_0-logloss:0.13568\n[4000]\tvalidation_0-auc:0.97281\tvalidation_0-logloss:0.13428\n[5000]\tvalidation_0-auc:0.97317\tvalidation_0-logloss:0.13337\n[6000]\tvalidation_0-auc:0.97343\tvalidation_0-logloss:0.13267\n[7000]\tvalidation_0-auc:0.97362\tvalidation_0-logloss:0.13216\n[8000]\tvalidation_0-auc:0.97373\tvalidation_0-logloss:0.13185\n[9000]\tvalidation_0-auc:0.97381\tvalidation_0-logloss:0.13161\n[10000]\tvalidation_0-auc:0.97386\tvalidation_0-logloss:0.13146\n[11000]\tvalidation_0-auc:0.97389\tvalidation_0-logloss:0.13137\n[12000]\tvalidation_0-auc:0.97390\tvalidation_0-logloss:0.13134\n[13000]\tvalidation_0-auc:0.97390\tvalidation_0-logloss:0.13134\n[14000]\tvalidation_0-auc:0.97391\tvalidation_0-logloss:0.13133\n[15000]\tvalidation_0-auc:0.97391\tvalidation_0-logloss:0.13133\n[15332]\tvalidation_0-auc:0.97391\tvalidation_0-logloss:0.13133\n[XGB-depthwise Fold 4] AUC=0.973911  LogLoss=0.131321  best_it=13833\n[0]\tvalidation_0-auc:0.94913\tvalidation_0-logloss:0.37761\n[1000]\tvalidation_0-auc:0.96866\tvalidation_0-logloss:0.14401\n[2000]\tvalidation_0-auc:0.97099\tvalidation_0-logloss:0.13861\n[3000]\tvalidation_0-auc:0.97204\tvalidation_0-logloss:0.13612\n[4000]\tvalidation_0-auc:0.97262\tvalidation_0-logloss:0.13468\n[5000]\tvalidation_0-auc:0.97301\tvalidation_0-logloss:0.13366\n[6000]\tvalidation_0-auc:0.97328\tvalidation_0-logloss:0.13295\n[7000]\tvalidation_0-auc:0.97346\tvalidation_0-logloss:0.13247\n[8000]\tvalidation_0-auc:0.97357\tvalidation_0-logloss:0.13213\n[9000]\tvalidation_0-auc:0.97364\tvalidation_0-logloss:0.13192\n[10000]\tvalidation_0-auc:0.97371\tvalidation_0-logloss:0.13173\n[11000]\tvalidation_0-auc:0.97374\tvalidation_0-logloss:0.13164\n[12000]\tvalidation_0-auc:0.97376\tvalidation_0-logloss:0.13159\n[13000]\tvalidation_0-auc:0.97375\tvalidation_0-logloss:0.13160\n[14000]\tvalidation_0-auc:0.97376\tvalidation_0-logloss:0.13159\n[14216]\tvalidation_0-auc:0.97375\tvalidation_0-logloss:0.13160\n[XGB-depthwise Fold 5] AUC=0.973761  LogLoss=0.131583  best_it=12717\n[0]\tvalidation_0-auc:0.94913\tvalidation_0-logloss:0.37762\n[1000]\tvalidation_0-auc:0.96863\tvalidation_0-logloss:0.14413\n[2000]\tvalidation_0-auc:0.97098\tvalidation_0-logloss:0.13867\n[3000]\tvalidation_0-auc:0.97206\tvalidation_0-logloss:0.13606\n[4000]\tvalidation_0-auc:0.97265\tvalidation_0-logloss:0.13454\n[5000]\tvalidation_0-auc:0.97300\tvalidation_0-logloss:0.13361\n[6000]\tvalidation_0-auc:0.97325\tvalidation_0-logloss:0.13293\n[7000]\tvalidation_0-auc:0.97339\tvalidation_0-logloss:0.13250\n[8000]\tvalidation_0-auc:0.97348\tvalidation_0-logloss:0.13222\n[9000]\tvalidation_0-auc:0.97355\tvalidation_0-logloss:0.13200\n[10000]\tvalidation_0-auc:0.97360\tvalidation_0-logloss:0.13184\n[11000]\tvalidation_0-auc:0.97363\tvalidation_0-logloss:0.13177\n[12000]\tvalidation_0-auc:0.97363\tvalidation_0-logloss:0.13175\n[13000]\tvalidation_0-auc:0.97363\tvalidation_0-logloss:0.13173\n[13822]\tvalidation_0-auc:0.97363\tvalidation_0-logloss:0.13175\n[XGB-depthwise Fold 6] AUC=0.973636  LogLoss=0.131726  best_it=12323\n[0]\tvalidation_0-auc:0.95376\tvalidation_0-logloss:0.37743\n[1000]\tvalidation_0-auc:0.97297\tvalidation_0-logloss:0.13410\n[2000]\tvalidation_0-auc:0.97414\tvalidation_0-logloss:0.13090\n[3000]\tvalidation_0-auc:0.97442\tvalidation_0-logloss:0.13001\n[4000]\tvalidation_0-auc:0.97451\tvalidation_0-logloss:0.12969\n[5000]\tvalidation_0-auc:0.97450\tvalidation_0-logloss:0.12969\n[6000]\tvalidation_0-auc:0.97446\tvalidation_0-logloss:0.12983\n[6136]\tvalidation_0-auc:0.97445\tvalidation_0-logloss:0.12986\n[XGB-lossguide Fold 1] AUC=0.974519  LogLoss=0.129642  best_it=4637\n[0]\tvalidation_0-auc:0.95348\tvalidation_0-logloss:0.37742\n[1000]\tvalidation_0-auc:0.97217\tvalidation_0-logloss:0.13584\n[2000]\tvalidation_0-auc:0.97351\tvalidation_0-logloss:0.13241\n[3000]\tvalidation_0-auc:0.97384\tvalidation_0-logloss:0.13148\n[4000]\tvalidation_0-auc:0.97394\tvalidation_0-logloss:0.13115\n[5000]\tvalidation_0-auc:0.97396\tvalidation_0-logloss:0.13109\n[6000]\tvalidation_0-auc:0.97396\tvalidation_0-logloss:0.13117\n[6816]\tvalidation_0-auc:0.97393\tvalidation_0-logloss:0.13134\n[XGB-lossguide Fold 2] AUC=0.973974  LogLoss=0.131075  best_it=5316\n[0]\tvalidation_0-auc:0.95342\tvalidation_0-logloss:0.37745\n[1000]\tvalidation_0-auc:0.97165\tvalidation_0-logloss:0.13683\n[2000]\tvalidation_0-auc:0.97282\tvalidation_0-logloss:0.13384\n[3000]\tvalidation_0-auc:0.97310\tvalidation_0-logloss:0.13308\n[4000]\tvalidation_0-auc:0.97319\tvalidation_0-logloss:0.13283\n[5000]\tvalidation_0-auc:0.97322\tvalidation_0-logloss:0.13282\n[6000]\tvalidation_0-auc:0.97320\tvalidation_0-logloss:0.13300\n[6048]\tvalidation_0-auc:0.97320\tvalidation_0-logloss:0.13300\n[XGB-lossguide Fold 3] AUC=0.973221  LogLoss=0.132787  best_it=4548\n[0]\tvalidation_0-auc:0.95413\tvalidation_0-logloss:0.37740\n[1000]\tvalidation_0-auc:0.97242\tvalidation_0-logloss:0.13531\n[2000]\tvalidation_0-auc:0.97360\tvalidation_0-logloss:0.13226\n[3000]\tvalidation_0-auc:0.97390\tvalidation_0-logloss:0.13140\n[4000]\tvalidation_0-auc:0.97400\tvalidation_0-logloss:0.13109\n[5000]\tvalidation_0-auc:0.97401\tvalidation_0-logloss:0.13109\n[6000]\tvalidation_0-auc:0.97399\tvalidation_0-logloss:0.13120\n[6201]\tvalidation_0-auc:0.97399\tvalidation_0-logloss:0.13124\n[XGB-lossguide Fold 4] AUC=0.974019  LogLoss=0.131048  best_it=4701\n[0]\tvalidation_0-auc:0.95371\tvalidation_0-logloss:0.37743\n[1000]\tvalidation_0-auc:0.97232\tvalidation_0-logloss:0.13556\n[2000]\tvalidation_0-auc:0.97349\tvalidation_0-logloss:0.13248\n[3000]\tvalidation_0-auc:0.97379\tvalidation_0-logloss:0.13162\n[4000]\tvalidation_0-auc:0.97388\tvalidation_0-logloss:0.13132\n[5000]\tvalidation_0-auc:0.97388\tvalidation_0-logloss:0.13131\n[5895]\tvalidation_0-auc:0.97386\tvalidation_0-logloss:0.13142\n[XGB-lossguide Fold 5] AUC=0.973894  LogLoss=0.131272  best_it=4396\n[0]\tvalidation_0-auc:0.95307\tvalidation_0-logloss:0.37745\n[1000]\tvalidation_0-auc:0.97233\tvalidation_0-logloss:0.13553\n[2000]\tvalidation_0-auc:0.97349\tvalidation_0-logloss:0.13244\n[3000]\tvalidation_0-auc:0.97375\tvalidation_0-logloss:0.13164\n[4000]\tvalidation_0-auc:0.97384\tvalidation_0-logloss:0.13131\n[5000]\tvalidation_0-auc:0.97381\tvalidation_0-logloss:0.13136\n[5946]\tvalidation_0-auc:0.97377\tvalidation_0-logloss:0.13150\n[XGB-lossguide Fold 6] AUC=0.973852  LogLoss=0.131273  best_it=4446\n============================================================\nOOF AUC (depthwise) = 0.9737758417212269\nOOF AUC (lossguide) = 0.9739106156585534\nOOF AUC (blend)     = 0.9739495289438653\nSaved: submission.csv / xgb_oof.csv / xgb_pred.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# === Fixed-weight blender: XGB(depthwise/lossguide) + LGBM ===\nimport os, numpy as np, pandas as pd\n\n# 1) test の id を取得\nPATH = \"/kaggle/input/playground-series-s5e8/\"\ntest = pd.read_csv(PATH + \"test.csv\", index_col=\"id\")\n\ndef try_read_pred(paths, prefer_cols=(\"y\",\"pred\",\"prediction\",\"xgb_pred\",\"lgb_pred\")):\n    \"\"\"最初に見つかったCSVから予測ベクトル(1D)を返す。列名が不明でも1列ならOK。\"\"\"\n    for p in paths:\n        if os.path.exists(p):\n            df = pd.read_csv(p)\n            for c in prefer_cols:\n                if c in df.columns:\n                    return df[c].values\n            if df.shape[1] == 1:\n                return df.iloc[:,0].values\n    return None\n\n# 2) ファイル候補（足りない名前も拾えるように多めに列挙）\nWRK = \"/kaggle/working\"\npred_lgb = try_read_pred([\n    f\"{WRK}/lgb_pred.csv\", f\"{WRK}/LGBM_pred.csv\", f\"{WRK}/pred_lgb.csv\",\n    f\"{WRK}/A-GBDT_pred.csv\", f\"{WRK}/B-GOSS_pred.csv\", f\"{WRK}/lgb_blend_pred.csv\",\n    # 最後に保存した submission がLGBMの可能性がある場合の保険（非推奨だが一応）\n    f\"{WRK}/submission_lgb.csv\"\n])\n\npred_xgb_dw = try_read_pred([\n    f\"{WRK}/xgb_depth_pred.csv\", f\"{WRK}/xgb_dw_pred.csv\", f\"{WRK}/xgb_depthwise_pred.csv\",\n    # depthwise単体を保存していない場合、2本平均のxgb_predは使わない（誤混入防止）\n])\n\npred_xgb_lg = try_read_pred([\n    f\"{WRK}/xgb_loss_pred.csv\", f\"{WRK}/xgb_lg_pred.csv\", f\"{WRK}/xgb_lossguide_pred.csv\",\n])\n\n# 3) 使えるモデルを集める（無いものはスキップ）\nmodels = []\nif pred_xgb_lg is not None: models.append((\"xgb_lossguide\", pred_xgb_lg))\nif pred_xgb_dw is not None: models.append((\"xgb_depthwise\", pred_xgb_dw))\nif pred_lgb    is not None: models.append((\"lgbm\", pred_lgb))\n\nassert len(models) >= 2, \"使える予測が2本以上見つかりません。保存CSVのパス名を確認してください。\"\n\n# 4) 固定重み（見つかったモデルに合わせて自動で正規化）\nfixed_w = {\n    \"xgb_lossguide\": 0.40,\n    \"xgb_depthwise\": 0.30,\n    \"lgbm\":          0.30,\n}\n# lossguideが無い場合は  depthwise 0.65 / lgbm 0.35 に置き換え\nif \"xgb_lossguide\" not in dict(models) and {\"xgb_depthwise\",\"lgbm\"}.issubset({k for k,_ in models}):\n    fixed_w = {\"xgb_depthwise\": 0.65, \"lgbm\": 0.35}\n\n# 有るモデルだけ抜き出して正規化\nw = np.array([fixed_w[mname] for mname,_ in models], dtype=float)\nw = w / w.sum()\n\n# 5) ブレンドして保存\npred = np.zeros(len(test), dtype=float)\nfor (mname, p), wi in zip(models, w):\n    pred += wi * p\n\nsub = pd.DataFrame({\"id\": test.index, \"y\": pred})\nsub.to_csv(f\"{WRK}/submission.csv\", index=False)\nprint(\"Used models & weights:\", [(m, float(wi)) for (m,_), wi in zip(models, w)])\nprint(\"Saved:\", f\"{WRK}/submission.csv  shape={sub.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T02:53:29.119824Z","iopub.execute_input":"2025-09-01T02:53:29.120098Z","iopub.status.idle":"2025-09-01T02:53:30.090298Z","shell.execute_reply.started":"2025-09-01T02:53:29.120075Z","shell.execute_reply":"2025-09-01T02:53:30.089638Z"}},"outputs":[{"name":"stdout","text":"Used models & weights: [('xgb_lossguide', 0.4), ('xgb_depthwise', 0.3), ('lgbm', 0.3)]\nSaved: /kaggle/working/submission.csv  shape=(250000, 2)\n","output_type":"stream"}],"execution_count":8}]}